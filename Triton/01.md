## triton 基础

### 1. triton的定义

Triton 既是语言，也是编译器。

#### 1.1 三大组件
- Intermediate Language（中间语言）：基于Python的DSL
- tiled Neural Network Compute（分块神经网络计算）： 面向GPU体系特点，自动分析和实施神经网络计算的分块
- Compiler（编译器）

#### 1.2 triton 特性
1. 粒度为 block(tile)：更关心 block（即 SM 和 CUDA 的 block 不完全一样），而不是 grid、block、thread 这样严格而又复杂的线程组织结构。
2. 优化 Pass: 借助一系列的优化 Pass，它可以达到和 cuBLAS 等算子库接近的水平。
3. 和 pytorch 无缝衔接: 输入是 torch 张量指针。

**triton 算子和 torch eager 算子区别：**

1. triton 的操作对象是张量指针，torch 的输入是张量视图。
  - 张量指针：关心张量布局，使用偏移量访问存储。
  - 张量视图：关心张量形状，可能在储存上不连续。

2. 通用算子领域，优化的 triton 算子性能可和 cuda 算子持平；在自定义算子、融合算子领域，短期内 triton 算子性能能更高。

#### 1.3 triton 编译过程

triton kernel -> triton IR -> LLVM IR -> PTX，最后配合 runtime 运行。

上述编译过程通过 @triton.jit 装饰器完成，具体来说是遍历提供的 Python 函数的抽象语法树（AST），并使用常见的 SSA 构建算法即时生成 Triton-IR。然后，编译器后端会简化、优化并自动并行化所产生的 IR 代码，再将其转换为高质量的 LLVM-IR，最后生成 PTX 并在 NVIDIA GPU 上执行。

#### 1.4 triton 保留关键字

| 关键字 | 作用 | 单位/范围 | 性能影响 |
|--------|------|----------|----------|
| `num_warps` | 设置 block 内 warp 数量 | warp 数（1-8） | block 内线程数 = num_warps × 32 |
| `num_stages` | 软件流水线阶段数 | 阶段数（1-3） | 隐藏内存延迟，仅 SM80+ |
| `num_ctas` | 每个 SM 并发 block 数 | block 数 | 提高 SM 占用率 |
| `grid` | 定义 block 网格结构 | 元组 `(x, y, z)` | 控制总并行度 |
| `enable_fp_fusion` | 浮点运算融合 | bool | 减少指令数，提升吞吐 |
| `maxnreg` | 每个 block 最大寄存器数 | 寄存器数 | 限制资源使用 |

#### 1.5 Pytorch 与 Triton 中的地址计算对比

<img width="1280" height="381" alt="image" src="https://github.com/user-attachments/assets/44919965-d9f6-4ead-8057-949bff6d6eef" />

#### 1.6 triton 和 cuda 特性的对比

Triton 将 CUDA 中需要手动处理的 Block 内优化（内存搬运、TensorCore 映射、向量化、同步）交给编译器自动完成，开发者只需关注 Block 间的任务划分。以灵活性换取开发效率。

| 特性 | CUDA | Triton |
|------|------|--------|
| **编程粒度** | Thread 级，手动管理线程布局 | Block 级，块内逻辑由编译器处理 |
| **内存管理** | 手动管理共享内存、全局内存搬运 | 编译器自动处理内存层级调度 |
| **TensorCore 调用** | 手动使用 `wmma`/`mma` 指令 | 编译器自动映射 `tl.dot` 到 TensorCore |
| **向量化** | 手动控制向量化加载/存储 | 编译器自动向量化 |
| **同步** | 手动插入 `__syncthreads()` | 编译器自动处理同步 |
| **开发效率** | 低（灵活但繁琐） | 高（抽象但受限） |
| **优化上限** | 高（完全可控） | 受编译器能力约束 |

#### 1.7 triton的 autotune 用法
