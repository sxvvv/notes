## Accelerating LLM Inference by Enabling Intermediate Layer Decoding

### 核心矛盾
  传统指令微调（IT）仅最终层具备高质量生成能力，中间层无法直接解码（生成质量差），导致无法通过 “早退” 减少计算量；

### 技术痛点：
  - 中间层表征无生成能力，强行早退会导致语义混乱、重复（论文附录 B.1 示例：Layer24 生成 “quantityquantityquantity” 无效文本）；
  
  - 固定早退机制（强制从某一层退出）无法平衡速度与质量，精度损失不可控；
  
  - 硬件 / 算子关联：中间层未被利用导致计算冗余，Transformer 解码器层的前向算子（Self-Attention、FFN）存在大量无效执行。

### 核心方案

通过 “LITE 指令微调（中间层显式损失注入）” 赋予中间层高质量生成能力，再通过 “token 级动态置信度早退” 机制，在推理时自适应判断是否从中间层退出，既减少后续层的冗余计算（Self-Attention、FFN、归一化等算子的重复执行），又通过置信度阈值保障生成质量与最终层一致。

<img width="486" height="176" alt="{8525BA01-3414-4CCD-9DEB-B32786954A19}" src="https://github.com/user-attachments/assets/4ae69318-5f49-4611-b6d8-5b3d354f1a7b" />

仅让中间层具备生成能力还不够 —— 固定从某一中间层退出（固定早停）会导致生成质量下降（中间层能力仍略逊于最后一层）。论文基于两个关键发现，设计了动态早停：

  - 两个核心发现：
    - LITE 微调后，中间层的 token 预测与最后一层高度 “对齐”（即中间层预测的 token 大概率和最后一层一致）；
    - 中间层的 token 预测置信度（logits 经 softmax 后的概率）是 “对齐程度” 的强信号（置信度越高，与最后一层预测一致的概率越高）。

动态早停逻辑（词元级 Token-level）：

  为每个选定的中间层设定专属置信度阈值（基于 “对齐度> 95%” 确定，如 Layer8：0.95、Layer28：0.7）；
  
  推理时，按层序（从浅到深）检查中间层的预测置信度：若某层置信度超过阈值，直接从该层退出并生成 token（跳过后续深层计算）；若未达标，则继续检查下一层，最终未达标的 token 仍由最后一层生成。
